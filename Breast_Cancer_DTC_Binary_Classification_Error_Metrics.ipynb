{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification_Error_Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will use decsion tree classification for a binary label classification task.\n",
    "\n",
    "# The classification model used is arbitrary the main focus is more on the error metrics.\n",
    "\n",
    "# Includes:\n",
    "\n",
    "# Confusions Matrix (2x2) inc true positives, true negatives, false positives, false negatives.\n",
    "# Overall accuracy aka classification accuracy\n",
    "# Misclassification Rate\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "# Specificity or true negative rate\n",
    "# Precision or positive predictive value\n",
    "# F1 Score\n",
    "# Negative predictive value\n",
    "# Fall out or false positive rate\n",
    "# False negative rate\n",
    "# False discovery rate\n",
    "# F1 Score\n",
    "# ROC curves\n",
    "# AUC\n",
    "# Mean Absolute Error\n",
    "# Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier,  plot_tree\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the sk learn breast cancer data set\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = pd.Series(cancer.target)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features are all the columns except the target (has breast cancer)\n",
    "X = df[df.columns[:-1]]\n",
    "\n",
    "# Target is the has breast cancer target column\n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "# Instatiate a decision tree classifier object\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model with the training X and y\n",
    "clf_dt = clf_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was actually 89 +ve cases of breast cancer\n",
      "There was actually 54 -ve cases of breast cancer\n",
      "\n",
      " The accuracy score when predicting the test data 0.951048951048951\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict the test data y_test\n",
    "y_test_pred = clf_dt.predict(X_test)\n",
    "\n",
    "# Then compare it to  the actual y_test data using this confusion matrix\n",
    "plot_confusion_matrix(clf_dt, X_test, y_test, display_labels=[\"No Cancer\", \"Has Cancer\"])\n",
    "\n",
    "# What is the accuracy of the model when it predicts the test Data (should be lower)  \n",
    "print('There was actually ' + str(y_test.sum()) + ' +ve cases of breast cancer')\n",
    "print('There was actually ' + str(len(y_test)-y_test.sum()) + ' -ve cases of breast cancer')\n",
    "print ('')\n",
    "print(f' The accuracy score when predicting the test data {accuracy_score(y_test_pred,y_test)}')\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Metrics and Confusion Matrix Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions can fall into one of 4 categories:\n",
    "\n",
    "TN = True Negative, correctly identifying the lack of event A\n",
    "\n",
    "FN = False Negative, incorrectly identifying the lack of event A\n",
    "\n",
    "TP = True Positive, correctly identiying event A as happening\n",
    "\n",
    "FP = False Positive, incorrectly identiying event A as happening\n",
    "\n",
    "We say that the positive case is the label/class/category that we are interested in that we are trying to predict.\n",
    "\n",
    "The negative is simply the alternative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a 2x2 stand bivariate classification task confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "CM = metrics.confusion_matrix(y_true = y_test, y_pred = y_test_pred)\n",
    "\n",
    "\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy aka classification accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n",
    "# Misclassification Rate\n",
    "MR = 1 - ACC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "\n",
    "# F1 Score\n",
    "F1 = 2 * ((PPV*TPR)/(PPV+TPR))\n",
    "\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# F1 Score\n",
    "F1 = 2 * ((PPV*TPR)/(PPV+TPR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Accuracy :\n",
    "\n",
    "Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples.\n",
    "\n",
    "The measure is a percentage of how many we correctly predicited. The true to all ratio.\n",
    "\n",
    "The problem with accuracy:\n",
    "\n",
    "It works well only if there are equal number of samples belonging to each class. Where this is not the case it can lead to a misleading judgment of error.\n",
    "\n",
    "![alt text](ACC.png \"Title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "# For our decision tree\n",
    "\n",
    "print(ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misclassifiaction Rate :\n",
    "\n",
    "\n",
    "\n",
    "The measure is a percentage of how many we incorrectly predicited. The false to all ratio.\n",
    "\n",
    "\n",
    "\n",
    "![alt text](MR.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04895104895104896\n"
     ]
    }
   ],
   "source": [
    "# For our decision tree\n",
    "\n",
    "MR = 1 - ACC\n",
    "\n",
    "print(MR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Positive Rate (Sensitivity aka Recall, Hit Rate ) : \n",
    "\n",
    "True Positive Rate is defined as TP/ (FN+TP). \n",
    "\n",
    "True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n",
    "\n",
    "\n",
    "![alt text](Sensitivity.gif \"Title\")\n",
    "\n",
    "Another way of defining sensitivity:\n",
    "\n",
    "The percentage that of True (1) events that actually happened and were correctly identified.\n",
    "\n",
    "\n",
    "The problem with sensitivity: It doesn't account for the TN or FP (negative (0)) instances that actually occured. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9550561797752809\n"
     ]
    }
   ],
   "source": [
    "# For our decision tree\n",
    "\n",
    "print(TPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Negative Rate (Specificity) :\n",
    "\n",
    "True Negative Rate is defined as TN / (FP+TN). \n",
    "\n",
    "True Negative Rate corresponds to the proportion of negative data points that are correctly predicted as negative, with respect to all negative data points.\n",
    "\n",
    "![alt text](TNR.png \"Title\")\n",
    "\n",
    "Another way of defining specificity:\n",
    "\n",
    "The percentage of False (0) events that actually did NOT happen that were correctly labelled as NOT.\n",
    "\n",
    "The percentage of False (0) events that where correctly labelled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# For our decision tree\n",
    "\n",
    "print(TNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specificity and Sensitivity\n",
    "\n",
    "They are good metrics for comparing models.\n",
    "\n",
    "You could use one model over the other based whether it is more important to be able to predict 0's or 1's events\n",
    "\n",
    "\n",
    "Specificity and sensitivity offer insight into whether a model is:\n",
    "\n",
    "better at predicting an event happening (Sensitivity)\n",
    "\n",
    "or\n",
    "\n",
    "better at predicting an event will not happen (Specificity)\n",
    "\n",
    "It could be that one is more important than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision AKA Positive Predictive Value:\n",
    "\n",
    "\n",
    "Precision answers the following, note all statements are synonymous:\n",
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. \n",
    "\n",
    "How precise the positive prediction is.\n",
    "\n",
    "How well you predict the label in question.\n",
    "\n",
    "How many of those that we predicted as true (1) were actually true (1)\n",
    "\n",
    "![alt text](P.png \"Title\")\n",
    "\n",
    "Note:\n",
    "\n",
    "Higher precision = fewer mistakes in making positive/true/(1) predictions\n",
    "\n",
    "Precision = 1 = perfect predictor = No false positives\n",
    "\n",
    "Issue: Negative labels are left out of the picture\n",
    "\n",
    "The problem with Precision: It doesn't account for the TN or FN instances (all of the negative (0) predicitions) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9659090909090909\n"
     ]
    }
   ],
   "source": [
    "# For our decision tree\n",
    "\n",
    "print(PPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall (Sensitivity)\n",
    "\n",
    "Both are good metrics yet, both can be misleading.\n",
    "\n",
    "With precision, negative labels are not considered.(Considers only +ve predictions TP,FP)\n",
    "\n",
    "With recall, falsely identified negative labels are considered (FN).\n",
    "\n",
    "Ideally in a good classifier, we want both precision and recall to be one. (No FP or FN)\n",
    "\n",
    "Which would mean 0 False positives and 0 false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 - Score :\n",
    "\n",
    "In a perfect classification model, precision and recall are one.\n",
    "\n",
    "F1 Score is the Harmonic Mean between precision and recall and is a better measure than accuracy.\n",
    "\n",
    "F1 score becomes high only when both precision and recall are high. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![alt text](F1.gif \"Title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96045197740113\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "# TPR\n",
    "\n",
    "# Precision\n",
    "# PPV\n",
    "\n",
    "# F1 Score\n",
    "F1 = 2 * ((PPV*TPR)/(PPV+TPR))\n",
    "\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error :\n",
    "\n",
    "What is mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04895104895104895"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "\n",
    "MAE = metrics.mean_absolute_error(y_true = y_test, y_pred = y_test_pred)\n",
    "\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Squared Error :\n",
    "\n",
    "What is mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04895104895104895"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Squared Error\n",
    "MSE = metrics.mean_squared_error(y_true = y_test, y_pred = y_test_pred)\n",
    "\n",
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to iterate through variations of fitted model.\n",
    "\n",
    "There are suitable metrics used to compare each iteration which use the count values for TN,TP,FP,FN.\n",
    "\n",
    "They are the ROC and AUC.\n",
    "\n",
    "Firstly though we need to become familiar with the FPR aka fallout rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positive Rate AKA Fallout Rate : \n",
    "\n",
    "False Positive Rate is defined as FP / (FP+TN).\n",
    "\n",
    "AKA \n",
    "\n",
    "FPR = 1 - specificity\n",
    "\n",
    "False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.\n",
    "\n",
    "![alt text](FP_Rate.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05555555555555555\n"
     ]
    }
   ],
   "source": [
    "print(FPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC :\n",
    "\n",
    "ROC is a Receiver Operator Characteristic Graph.\n",
    "\n",
    "Say you have a DTC model and you want to alter a single parameter and see how a range of values of it can change the quality of the model.\n",
    "\n",
    "Instead of producing a confusion matrix for every tree:\n",
    "\n",
    "We plot a set of TPR (Sensitivity) values against FPR (1 - specificity) values, a data point for each model.\n",
    "\n",
    "\n",
    "The data points with the highest TPR and lowest FPR should be considered as the optimal value for the parameter.\n",
    "\n",
    "Usually this is the one in the nearest to the top left corner.\n",
    "\n",
    "Why? because a perfect classifier would have 1 TPR and 0 FPR.\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "![alt text](ROC.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area Under Curve :\n",
    "Area Under Curve (AUC) is one of the most widely used metrics for evaluation.\n",
    "\n",
    "It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.\n",
    "\n",
    "It is the area under the ROC curve\n",
    "\n",
    "It is good for comparing one ROC curve to another.\n",
    "\n",
    "Bigger AUC = Better ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
