{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAST CANCER PREDICTION - DTC and CCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here DTC classification has been used to build a model which can predict whether a patient has or hasn't got breast cancer using 30 different feature variables.\n",
    "\n",
    "Decision trees are notorius for overfitting where the leaf nodes are all split until purity. It is standard to prune them, as such they generalize better to unseen data.\n",
    "\n",
    "Cost complexity pruning is one approach for this. Whereby, parameter alpha (a hyperparameter that encapsulates multiple pruning parameters such as max_depth, min_samples etc.) is optimized for. This is done via iteration of different models where error metrics are calculated to perform a comparitive analysis of the different alpha values and the models they generate.\n",
    "\n",
    "Part One: Making an initial model with default parameters set in decision tree classifier model.\n",
    "\n",
    "Part Two: Cost complexity pruning. Optimization of model parameter alpha\n",
    "\n",
    "    i: Comparing possible alpha values via their models accuracy score on the test data. Where multiple alphas return the\n",
    "    same mean accuracy score over 5-fold cross validation, the highest f1_score of the remaining is selected.\n",
    "\n",
    "    ii: Comparing via f1_Score alone. Which is possibly a better metric, given that accuracy can be mis-leading depending on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV,cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score,confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import tree\n",
    "\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(clf_dt, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Returns f1-score for a set of predicitions against actual results.\n",
    "    \"\"\"\n",
    "    y_test_pred = clf_dt.predict(X_test)\n",
    "    CM = metrics.confusion_matrix(y_true = y_test, y_pred = y_test_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    PPV = TP/(TP+FP)\n",
    "    TPR = TP/(TP+FN)\n",
    "    F1 = 2 * ((PPV*TPR)/(PPV+TPR))\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_act_acc_f1(dt, y, X, pos_label, neg_label):\n",
    "    \n",
    "    \"\"\" \n",
    "        Returns:\n",
    "        a confusion matrix\n",
    "        f1-score\n",
    "        average f1-score over 5-fold cross validation\n",
    "        \n",
    "        accuracy score\n",
    "        average accuracy score over 5-fold cross validation\n",
    "        \n",
    "        Numbers of +ve and -ve observations\n",
    "    \"\"\"\n",
    "    \n",
    "    y_train_pred = dt.predict(X)\n",
    "    \n",
    "    print(f' The f1 - score: {f1_score(dt, X, y)}')\n",
    "    scores = cross_val_score(dt, X, y, scoring=\"f1\", cv = 5)\n",
    "    print(scores)\n",
    "    print(f' The average f1 - score after 5-fold CV: {scores.mean()}')\n",
    "    \n",
    "    print('')\n",
    "    print(f' The accuracy score: {accuracy_score(  y_train_pred ,  y)}')\n",
    "    scores2 = cross_val_score(dt, X, y, cv = 5)\n",
    "    print(scores2)\n",
    "    print(f' The average accuracy - score after 5-fold CV: {scores2.mean()}')\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    print('There was actually ' + str(y.sum()) + ' +ve Observations')\n",
    "    print('There was actually ' + str(len(y)-y.sum()) + ' -ve Observations')\n",
    "    \n",
    "    plot_confusion_matrix(dt, X, y, display_labels=[neg_label, pos_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one: Making an initial model ------ START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sk learn breast cancer data set\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Put it into a Pandas dataframe\n",
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = pd.Series(cancer.target)\n",
    "df\n",
    "\n",
    "# The features columns are all of them except the target (has breast cancer)\n",
    "X = df[df.columns[:-1]]\n",
    "# The target is the has breast cancer target column\n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                float64\n",
       "mean texture               float64\n",
       "mean perimeter             float64\n",
       "mean area                  float64\n",
       "mean smoothness            float64\n",
       "mean compactness           float64\n",
       "mean concavity             float64\n",
       "mean concave points        float64\n",
       "mean symmetry              float64\n",
       "mean fractal dimension     float64\n",
       "radius error               float64\n",
       "texture error              float64\n",
       "perimeter error            float64\n",
       "area error                 float64\n",
       "smoothness error           float64\n",
       "compactness error          float64\n",
       "concavity error            float64\n",
       "concave points error       float64\n",
       "symmetry error             float64\n",
       "fractal dimension error    float64\n",
       "worst radius               float64\n",
       "worst texture              float64\n",
       "worst perimeter            float64\n",
       "worst area                 float64\n",
       "worst smoothness           float64\n",
       "worst compactness          float64\n",
       "worst concavity            float64\n",
       "worst concave points       float64\n",
       "worst symmetry             float64\n",
       "worst fractal dimension    float64\n",
       "target                       int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formatting the  data\n",
    "# See if there is anything unexpected.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "# Instatiate a decision tree classifier object  ### This is the initial model to be later optimised\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model with the training X and y\n",
    "clf_dt = clf_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The f1 - score: 1.0\n",
      "[0.94545455 0.91428571 0.93069307 0.94444444 0.94444444]\n",
      " The average f1 - score after 5-fold CV: 0.935864443587216\n",
      "\n",
      " The accuracy score: 1.0\n",
      "[0.93023256 0.89411765 0.91764706 0.94117647 0.91764706]\n",
      " The average accuracy - score after 5-fold CV: 0.9201641586867305\n",
      "\n",
      "There was actually 268 +ve Observations\n",
      "There was actually 158 -ve Observations\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model:\n",
    "\n",
    "# Predict the training data it should be 100% Accurate as it is the same data that the model was fit to.\n",
    "\n",
    "# We trained the model using X_train and y_train\n",
    "\n",
    "# We now predict y_train from \"X_train\" alone using the clf_dt model\n",
    "\n",
    "# predict the training data y_train\n",
    "\n",
    "cm_act_acc_f1(dt = clf_dt, y = y_train, X= X_train, pos_label = 'Has BC', neg_label= 'No BC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The f1 - score: 0.9540229885057471\n",
      "[0.97297297 0.94444444 0.94736842 0.94117647 0.91428571]\n",
      " The average f1 - score after 5-fold CV: 0.9440496046687998\n",
      "\n",
      " The accuracy score: 0.9440559440559441\n",
      "[0.93103448 0.96551724 0.93103448 0.92857143 0.85714286]\n",
      " The average accuracy - score after 5-fold CV: 0.9226600985221675\n",
      "\n",
      "There was actually 89 +ve Observations\n",
      "There was actually 54 -ve Observations\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the TESTING data accuracy and it should be less than 100%\n",
    "\n",
    "# We trained the model using X_train and y_train\n",
    "# We now predict \"y_test\" from \"X_test\" using the clf_dt model\n",
    "\n",
    "# predict the test daya y_test\n",
    "cm_act_acc_f1(dt = clf_dt, y = y_test, X= X_test, pos_label = 'Has BC', neg_label= 'No BC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9540229885057471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the F1 -Score of this predictive model\n",
    "f1_score(clf_dt, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9317653847067648"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The average F1-Score of this model over 5 fold validation is\n",
    "scores = cross_val_score(clf_dt, X_train, y_train, scoring=\"f1\", cv = 5)\n",
    "scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29b96f03548>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check how much of a fluke this f1 score is by doing 5 fold cross validation\n",
    "df = pd.DataFrame(data = {'tree': range(5), 'F1 Score': scores})\n",
    "df.plot(x = 'tree', y= 'F1 Score', marker = 'o', ls ='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2d44b06021b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'BC'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'No BC'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# How to know the order.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_dt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_export.py\u001b[0m in \u001b[0;36mplot_tree\u001b[1;34m(decision_tree, max_depth, feature_names, class_names, label, filled, impurity, node_ids, proportion, rotate, rounded, precision, ax, fontsize)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mproportion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproportion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrounded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrounded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         precision=precision, fontsize=fontsize)\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mexporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_export.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(self, decision_tree, ax)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         my_tree = self._make_tree(0, decision_tree.tree_,\n\u001b[1;32m--> 585\u001b[1;33m                                   decision_tree.criterion)\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[0mdraw_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuchheim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_export.py\u001b[0m in \u001b[0;36m_make_tree\u001b[1;34m(self, node_id, et, criterion, depth)\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;31m# traverses _tree.Tree recursively, builds intermediate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[1;31m# \"_reingold_tilford.Tree\" object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_to_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0met\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m         if (et.children_left[node_id] != _tree.TREE_LEAF\n\u001b[0;32m    567\u001b[0m                 and (self.max_depth is None or depth <= self.max_depth)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_export.py\u001b[0m in \u001b[0;36mnode_to_str\u001b[1;34m(self, tree, node_id, criterion)\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;31m# Always write node decision criteria, except for leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                 \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 feature = \"X%s%s%s\" % (characters[1],\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3928\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3929\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3930\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3932\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 7 is out of bounds for axis 0 with size 2"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we will viualise the decision tree where I think all terminal leaf nodes should be pure.\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "features = df.columns\n",
    "classes = ['BC','No BC'] # How to know the order.\n",
    "tree.plot_tree(clf_dt,feature_names=features,class_names=classes,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part one: Making an initial model ------ END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part two i: Optimization of model finding the best parameter alpha.\n",
    "\n",
    "# Done via first optmizing for accuracy\n",
    "# Then for F1 score (where multiple top accuracies are found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------- Finding the possible alphas : start ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the possible alphas and corresponding impurity in the terminal leaf nodes\n",
    "\n",
    "path = clf_dt.cost_complexity_pruning_path(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualise this\n",
    "\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(ccp_alphas,impurities)\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"total impurity of terminal leaf nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the alphas -returns in array format\n",
    "ccp_alphas = path.ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the last alpha -(which is just the root)\n",
    "ccp_alphas = ccp_alphas[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------- Finding the possible alphas : End ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using accuracy as a quality metric ----- START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this by making a decision tree for every alpha\n",
    "\n",
    "# List of decision trees - There will be one for every possible alpha\n",
    "clf_dts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run a loop for all of the alphas and put all of the DT into an array\n",
    "\n",
    "# We make a list of DT for every alpha\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf_dt = DecisionTreeClassifier(random_state = 0, ccp_alpha = ccp_alpha)\n",
    "    clf_dt.fit(X_train,y_train)\n",
    "    clf_dts.append(clf_dt)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find the accuracy for every DT on the training data and test data for each\n",
    "# of the new alpha decision trees\n",
    "\n",
    "train_scores =  [         clf_dt.score(X_train, y_train)    for clf_dt      in clf_dts       ]\n",
    "test_scores =  [         clf_dt.score(X_test, y_test)    for clf_dt      in clf_dts       ]\n",
    "\n",
    "alpha_df = pd.DataFrame(ccp_alphas, columns = [\"Alphas\"])\n",
    "train_scores_df = pd.DataFrame(train_scores, columns = [\"Train Score\"])\n",
    "test_scores_df = pd.DataFrame(test_scores, columns = [\"Test Score\"])\n",
    "\n",
    "\n",
    "alpha_frame = alpha_df.join( [train_scores_df , test_scores_df] )\n",
    "alpha_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the above\n",
    "ax = plt.subplot()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alphas(Training and Testing Sets)\")\n",
    "ax.plot(ccp_alphas, train_scores, marker = 'o' , label = \"train\" , drawstyle = \"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker = 'o' , label = \"test\" , drawstyle = \"steps-post\")\n",
    "\n",
    "ax.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From inspection above you can see that the accuracy of the model\n",
    "# looks best where the test line peaks\n",
    "# This may not be the optimal value\n",
    "# We use cross validation and different folds to work out what the best alpha is\n",
    "\n",
    "# Using a different number of folds - is splitting the data into training and testing in different ways\n",
    "\n",
    "# CROSS VALIDATION TO FIND THE BEST ALPHA\n",
    "\n",
    "# An array to store the results of each fold\n",
    "alpha_loop_values = []\n",
    "\n",
    "\n",
    "# We then run a loop for all of the alphas with a 5 fold cross validation\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    \n",
    "    clf_dt = DecisionTreeClassifier(random_state = 0, ccp_alpha = ccp_alpha)\n",
    "    clf_dt.fit(X_train,y_train)\n",
    "    \n",
    "    # Produce a set of accuracy scores for 5 folds\n",
    "    scores = cross_val_score(clf_dt, X_train, y_train, cv = 5)\n",
    "    \n",
    "    # Add to the array the an alpha column mean cross_val_score\n",
    "    alpha_loop_values.append([  ccp_alpha  ,  np.mean(scores)  ,  np.std(scores) ])\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_results = pd.DataFrame(alpha_loop_values, columns = ['alpha', 'mean_accuracy' ,  'std' ])\n",
    "alpha_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_results.plot(x=\"alpha\",y='mean_accuracy', yerr = 'std' , marker ='o', ls = '--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################################################\n",
    "# Find the highest accuracy ALPHA\n",
    "a = alpha_results.loc[alpha_results['mean_accuracy'] == alpha_results['mean_accuracy'].max(),'alpha']\n",
    "\n",
    "\n",
    "best_alphas = alpha_results[alpha_results['mean_accuracy'] == alpha_results['mean_accuracy'].max()].copy()\n",
    "\n",
    "best_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this has resulted in 3 alphas being found each with the same mean accuracy score and std. dev. for each of there models. We therefore need another error metric to find which of the remaining is the best. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using accuracy a a quality metric ----- END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through the best alphas to find the one with the lowest f1 score. (Additional, likely not needed with real data)---START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "f1s = []\n",
    "\n",
    "for alpha in best_alphas['alpha']:\n",
    "    clf_dt = DecisionTreeClassifier(random_state = 0, ccp_alpha = alpha)\n",
    "    clf_dt.fit(X_train,y_train)\n",
    "    \n",
    "    y_test_pred = clf_dt.predict(X_test)\n",
    "    \n",
    "    CM = metrics.confusion_matrix(y_true = y_test, y_pred = y_test_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    PPV = TP/(TP+FP)\n",
    "    TPR = TP/(TP+FN)\n",
    "    F1 = 2 * ((PPV*TPR)/(PPV+TPR))\n",
    "    \n",
    "    f1s.append(F1)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alphas['F1s'] = f1s\n",
    "\n",
    "best_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_ccp_alpha = best_alphas[best_alphas['F1s'] == best_alphas['F1s'].max()]['alpha'].max()\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# I think this will return an error on randint generated data sets with\n",
    "# there being so little difference in mean accuracy between alphas\n",
    "\n",
    "\n",
    "# THESE MAY NEED TO BE INDIVIDUALLY STUDIED\n",
    "\n",
    "ideal_ccp_alpha\n",
    "\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# REPLOT WITH THE ACCURATE ALPHA\n",
    "clf_dt_pruned = DecisionTreeClassifier(random_state=4, ccp_alpha = ideal_ccp_alpha)\n",
    "clf_dt_pruned = clf_dt_pruned.fit(X_train, y_train)\n",
    "\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# The Tree\n",
    "plt.figure(figsize=(15,7.5))\n",
    "tree.plot_tree(clf_dt_pruned,feature_names=features,class_names=classes,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training data it should be 100%  \n",
    "# - Not really need but provides a sanity test and could show if any mistakes have crept up\n",
    "\n",
    "# We trained the model using X_train and y_train\n",
    "\n",
    "# Then compare it to y_test data using this confusion matrix\n",
    "\n",
    "plot_confusion_matrix(clf_dt_pruned, X_train, y_train, display_labels=[\"No BC\", \"BC\"])\n",
    "\n",
    "# predict the training data y_train\n",
    "y_train_pred = clf_dt_pruned.predict(X_train)\n",
    "\n",
    "# What is the accuracy of the model when it predicts the ___ Training Data\n",
    "print ('')\n",
    "print(f' The accuracy score when predicting the training data (should be high) here it is {accuracy_score(  y_train_pred ,  y_train)}')\n",
    "print ('')\n",
    "print('There was actually ' + str(y_train.sum()) + ' +ve BC Observations')\n",
    "print('There was actually ' + str(len(y_train)-y_train.sum()) + ' -ve BC Observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the TESTING data it should be less than 100%\n",
    "\n",
    "# We trained the model using X_train and y_train\n",
    "\n",
    "# We now predict y from \"X_test\" using the clf_dt model\n",
    "\n",
    "# predict the test daya y_test\n",
    "y_test_pred = clf_dt_pruned.predict(X_test)\n",
    "\n",
    "# Then compare it to y_test data using this confusion matrix\n",
    "plot_confusion_matrix(clf_dt_pruned, X_test, y_test, display_labels=[\"No BC\", \"BC\"])\n",
    "\n",
    "\n",
    "\n",
    "# What is the accuracy of the model when it predicts the ___ Test Data\n",
    "print ('')\n",
    "print(f' The accuracy score when predicting the test data (should be lower)  {accuracy_score(y_test_pred,y_test)}')\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1-score for this model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1_score(clf_dt_pruned, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_ccp_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through the best alphas to find the one with the lowest f1 score. (Additional, likely not needed with real data)---END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best alpha has now been found\n",
    "\n",
    "The above tree is now optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THOUGHT\n",
    "\n",
    "# It may be better to optimize for f1 alone and not accuracy at all during the pruning stage given data input may have an unequal amount of postive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 - Get predictions = your final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After collecting data an extra column will need to be added\n",
    "# it will be a show of that are about to flood based on previous flood.\n",
    "# {No flood, about to flood, flooded}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding best alpha with just f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this by making a decision tree for every alpha\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of decision trees - There will be one for every possible alpha\n",
    "clf_dts = []\n",
    "\n",
    "# We run a loop for all of the alphas and put all of the DT into an array\n",
    "\n",
    "# We make a list of DT for every alpha\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf_dt = DecisionTreeClassifier(random_state = 0, ccp_alpha = ccp_alpha)\n",
    "    clf_dt.fit(X_train,y_train)\n",
    "    clf_dts.append(clf_dt)\n",
    "    pass\n",
    "\n",
    "\n",
    "# We find the F1 SCORE for every DT on the training data and test data for each\n",
    "# of the new alpha decision trees\n",
    "\n",
    "def f1_score(clf_dt, X_test, y_test):\n",
    "    y_test_pred = clf_dt.predict(X_test)\n",
    "    CM = metrics.confusion_matrix(y_true = y_test, y_pred = y_test_pred)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    PPV = TP/(TP+FP)\n",
    "    TPR = TP/(TP+FN)\n",
    "    F1 = 2 * ((PPV*TPR)/(PPV+TPR))\n",
    "    return F1\n",
    "\n",
    "\n",
    "train_scores =  [f1_score(clf_dt, X_train, y_train)    for clf_dt      in clf_dts       ]\n",
    "test_scores =  [f1_score(clf_dt, X_test, y_test)    for clf_dt      in clf_dts       ]\n",
    "\n",
    "alpha_df = pd.DataFrame(ccp_alphas, columns = [\"Alphas\"])\n",
    "train_scores_df = pd.DataFrame(train_scores, columns = [\"Train F1 Score\"])\n",
    "test_scores_df = pd.DataFrame(test_scores, columns = [\"Test F1 Score\"])\n",
    "\n",
    "alpha_frame = alpha_df.join( [train_scores_df , test_scores_df] )\n",
    "alpha_frame\n",
    "\n",
    "# Plotting the above\n",
    "ax = plt.subplot()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\"F1 Score vs alphas (Training and Testing Sets)\")\n",
    "ax.plot(ccp_alphas, train_scores, marker = 'o' , label = \"train\" , drawstyle = \"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker = 'o' , label = \"test\" , drawstyle = \"steps-post\")\n",
    "ax.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From inspection above you can see that the accuracy of the model\n",
    "# looks best where the test line peaks\n",
    "# This may not be the optimal value\n",
    "# We use cross validation and different folds to work out what the best alpha is\n",
    "\n",
    "# With 5-fold cross validation we create 5 different training and testing datasets\n",
    "# that are used to train and test the tree\n",
    "\n",
    "\n",
    "# Example explanation:\n",
    "# Below returns a list of 5 scores for a decision tree for the tenth alpha\n",
    "clf_dt = DecisionTreeClassifier(random_state = 0, ccp_alpha = ccp_alphas[10])\n",
    "\n",
    "scores = cross_val_score(clf_dt, X_train, y_train, scoring=\"f1\" , cv = 5)\n",
    "df = pd.DataFrame(data = {'tree': range(5), 'F1 Score': scores})\n",
    "df.plot(x = 'tree', y= 'F1 Score', marker = 'o', ls ='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From inspection above you can see that the F1 score of the model\n",
    "# varies for the same alpha depending on how the data is split\n",
    "\n",
    "\n",
    "# We use cross validation and different folds to work out what the best alpha is\n",
    "# by averaging the f1 score for each 5 of the folds per alpha\n",
    "\n",
    "\n",
    "# CROSS VALIDATION TO FIND THE BEST ALPHA\n",
    "\n",
    "# An array to store the results of each fold\n",
    "alpha_loop_values = []\n",
    "\n",
    "\n",
    "def range_of_vals(x, axis=0):\n",
    "    return np.max(x, axis=axis) - np.min(x, axis=axis)\n",
    "\n",
    "# We then run a loop for all of the alphas with a 5 fold cross validation\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf_dt = DecisionTreeClassifier(random_state = 0, ccp_alpha = ccp_alpha)\n",
    "    clf_dt.fit(X_train,y_train)\n",
    "    \n",
    "    # Produce a set of F1-Score scores for 5 folds\n",
    "    scores = cross_val_score(clf_dt, X_train, y_train, scoring=\"f1\" , cv = 5)\n",
    "    # Add to the array the an alpha column mean cross_val_score\n",
    "    alpha_loop_values.append([  ccp_alpha  ,  np.mean(scores)  ,  np.std(scores), range_of_vals(scores) ])\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "alpha_results = pd.DataFrame(alpha_loop_values, columns = ['alpha', 'mean_f1_score' ,  'std', 'range' ])\n",
    "alpha_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_results.plot(x=\"alpha\",y='mean_f1_score', yerr = 'range' , marker ='o', ls = '--')\n",
    "alpha_results.plot(x=\"alpha\",y='mean_f1_score', yerr = 'std' , marker ='o', ls = '--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though the above mean f1 scores can have considerable std and range...\n",
    "# ... Note that they may be insignicant when it comes to real data sets.\n",
    "# ... It is likely that the best model will be that with the highest average f1-score\n",
    "\n",
    "###########################################################################\n",
    "# Find the highest mean F1 scoring ALPHA\n",
    "a = alpha_results.loc[alpha_results['mean_f1_score'] == alpha_results['mean_f1_score'].max(),'alpha']\n",
    "\n",
    "\n",
    "best_alphas = alpha_results[alpha_results['mean_f1_score'] == alpha_results['mean_f1_score'].max()].copy()\n",
    "\n",
    "best_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_ccp_alpha = best_alphas[best_alphas['mean_f1_score'] == best_alphas['mean_f1_score'].max()]['alpha'].max()\n",
    "\n",
    "###########################################################################\n",
    "# REPLOT WITH THE ACCURATE ALPHA\n",
    "clf_dt_pruned = DecisionTreeClassifier(random_state=4, ccp_alpha = ideal_ccp_alpha)\n",
    "clf_dt_pruned = clf_dt_pruned.fit(X_train, y_train)\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "# Plot\n",
    "# The Tree\n",
    "plt.figure(figsize=(15,7.5))\n",
    "tree.plot_tree(clf_dt_pruned,feature_names=features,class_names=classes,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the training data it should be 100%  \n",
    "# - Not really need but provides a sanity test and could show if any mistakes have crept up\n",
    "\n",
    "# We trained the model using X_train and y_train\n",
    "\n",
    "\n",
    "# Then compare it to y_test data using this confusion matrix\n",
    "\n",
    "plot_confusion_matrix(clf_dt_pruned, X_train, y_train, display_labels=[\"No BC\", \"BC\"])\n",
    "\n",
    "# predict the training data y_train\n",
    "y_train_pred = clf_dt_pruned.predict(X_train)\n",
    "\n",
    "# What is the accuracy of the model when it predicts the ___ Training Data\n",
    "print ('')\n",
    "print(f' The accuracy score when predicting the training data (should be high) here it is {accuracy_score(  y_train_pred ,  y_train)}')\n",
    "print ('')\n",
    "print('There was actually ' + str(y_train.sum()) + ' +ve BC Observations')\n",
    "print('There was actually ' + str(len(y_train)-y_train.sum()) + ' -ve BC Observations')\n",
    "\n",
    "# Predict the TESTING data it should be less than 100%\n",
    "\n",
    "# We trained the model using X_train and y_train\n",
    "\n",
    "# We now predict y from \"X_test\" using the clf_dt model\n",
    "\n",
    "# predict the test daya y_test\n",
    "y_test_pred = clf_dt_pruned.predict(X_test)\n",
    "\n",
    "# Then compare it to y_test data using this confusion matrix\n",
    "plot_confusion_matrix(clf_dt_pruned, X_test, y_test, display_labels=[\"No BC\", \"BC\"])\n",
    "\n",
    "\n",
    "\n",
    "# What is the accuracy of the model when it predicts the ___ Test Data\n",
    "print ('')\n",
    "print(f' The accuracy score when predicting the test data (should be lower)  {accuracy_score(y_test_pred,y_test)}')\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(clf_dt_pruned, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this final result of the error metrics here is lower than that of the original initial iteration.\n",
    "\n",
    "# One might presume that the orginal split was an overfit fluke.\n",
    "# The final f1_score may be lower but it will generalise better to new unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is f1_score a suitable metric?\n",
    "\n",
    "One problem with f1_score is that is doesn't take True Negatives (TN) into consideration\n",
    "\n",
    "To a quick check here can be done by seeing what percentage of the total predicitions where true negatives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of predictions that are true negative:\n",
    "\n",
    "y_test_pred = clf_dt_pruned.predict(X_test)\n",
    "CM = metrics.confusion_matrix(y_true = y_test, y_pred = y_test_pred)\n",
    "TN = CM[0][0]\n",
    "FN = CM[1][0]\n",
    "TP = CM[1][1]\n",
    "FP = CM[0][1]\n",
    "\n",
    "perc = TN/(TN+FN+TP+FP)\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
